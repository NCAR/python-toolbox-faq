{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from subprocess import call\n",
    "from glob import glob\n",
    "import yaml\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "import cftime\n",
    "from datetime import datetime\n",
    "\n",
    "USER = os.environ['USER']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = os.environ['PBS_ACCOUNT']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spin up dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    from dask.distributed import Client\n",
    "    from dask_jobqueue import PBSCluster\n",
    "\n",
    "    project = PROJECT\n",
    "\n",
    "    cluster = PBSCluster(queue='economy',\n",
    "                         cores = 36,\n",
    "                         processes = 1,\n",
    "                         memory = '100GB',          \n",
    "                         project = project,\n",
    "                         walltime = '02:00:00',\n",
    "                         local_directory=f'/glade/scratch/{USER}/dask-tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<distributed.deploy.adaptive.Adaptive at 0x2aaae184f198>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.adapt(minimum=5, maximum=20)  # auto-scale between 20 and 50 workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "chadmin1: \n",
      "                                                            Req'd  Req'd   Elap\n",
      "Job ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time\n",
      "--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----\n",
      "3139991.chadmin abanihi  economy  STDIN       48467   1   1    --  06:00 R 00:36\n",
      "3140098.chadmin abanihi  economy  dask-worke  29007   1   1    --  02:00 R 00:00\n",
      "3140099.chadmin abanihi  economy  dask-worke  37364   1   1    --  02:00 R 00:00\n",
      "3140100.chadmin abanihi  economy  dask-worke  11972   1   1    --  02:00 R 00:00\n",
      "3140101.chadmin abanihi  economy  dask-worke    --    1   1    --  02:00 Q   -- \n",
      "3140102.chadmin abanihi  economy  dask-worke    --    1   1    --  02:00 Q   -- \n",
      "3140104.chadmin abanihi  economy  dask-worke    --    1   1    --  02:00 Q   -- \n"
     ]
    }
   ],
   "source": [
    "!qstat -u $USER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://10.148.2.69:46251\n",
       "  <li><b>Dashboard: </b><a href='http://10.148.2.69:8787/status' target='_blank'>http://10.148.2.69:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>5</li>\n",
       "  <li><b>Cores: </b>180</li>\n",
       "  <li><b>Memory: </b>500.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://10.148.2.69:46251' processes=5 cores=180>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what to do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 'O2' # O2 (3D var), HMXL (2D var)\n",
    "varout = var\n",
    "isel = {}\n",
    "\n",
    "#varout = 'npac_O2'\n",
    "#isel = {'nlat':slice(187,331),'nlon':slice(137,276)}\n",
    "\n",
    "rename_var = False\n",
    "\n",
    "n_ensemble_max = 2 # use only the first `n_ensemble_max` members\n",
    "\n",
    "mean = 'ann'\n",
    "n_forecast_lead = 10 # related to expected freq from \"mean\"\n",
    "\n",
    "diro = f'/glade/scratch/{USER}/calcs/o2-prediction'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## more advanced options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_format = 'nc' # nc or zarr for output\n",
    "\n",
    "file_out = {'mean': f'{diro}/CESM-DP-LE.{varout}.{mean}.mean.{file_format}',\n",
    "            'drift': f'{diro}/CESM-DP-LE.{varout}.{mean}.mean.drift.{file_format}',\n",
    "            'anom': f'{diro}/CESM-DP-LE.{varout}.{mean}.mean.anom.{file_format}'}\n",
    "\n",
    "first_start_year, last_start_year = 2011, 2015\n",
    "first_clim_year, last_clim_year = 2013, 2014\n",
    "\n",
    "dir_dple = '/glade/p_old/decpred/CESM-DPLE'\n",
    "case_prefix = 'b.e11.BDP.f09_g16'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define function for writing output based on file extension (zarr or nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output(ds,file_out,attrs={}):\n",
    "    '''Function to write output:\n",
    "       - optionally add some file-level attrs\n",
    "       - switch method based on file extension\n",
    "       '''\n",
    "    \n",
    "    diro = os.path.dirname(file_out)\n",
    "    if not os.path.exists(diro):\n",
    "        call(['mkdir','-p',diro])\n",
    "\n",
    "    if os.path.exists(file_out):\n",
    "        call(['rm','-fr',file_out])          \n",
    "\n",
    "    if attrs:\n",
    "        ds.attrs = attrs\n",
    "       \n",
    "    ext = os.path.splitext(file_out)[1]    \n",
    "    if ext == '.nc':\n",
    "        print(f'writing {file_out}')\n",
    "        ds.to_netcdf(file_out,compute=True)\n",
    "        \n",
    "    elif ext == '.zarr':\n",
    "        print(f'writing {file_out}')\n",
    "        ds.to_zarr(file_out,compute=True)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('Unknown output file extension: {ext}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some helper functions for reading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_bound_var(ds):\n",
    "    tb_name = ''\n",
    "    if 'bounds' in ds['time'].attrs:\n",
    "        tb_name = ds['time'].attrs['bounds']\n",
    "    elif 'time_bound' in ds:\n",
    "        tb_name = 'time_bound'\n",
    "    else:\n",
    "        raise ValueError('No time_bound variable found')\n",
    "    tb_dim = ds[tb_name].dims[-1]\n",
    "    return tb_name,tb_dim\n",
    "\n",
    "def fix_time(ds):\n",
    "    tb_name,tb_dim = time_bound_var(ds)\n",
    "    time = cftime.num2date(ds[tb_name].isel(M=0).mean(tb_dim),\n",
    "                           units = ds.time.attrs['units'],\n",
    "                           calendar = ds.time.attrs['calendar'])\n",
    "\n",
    "    ds.time.values = time\n",
    "    ds = ds.drop([tb_name])\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute drift\n",
    "Drift is assessed by computing a lead-time dependent climatology\n",
    "\n",
    "1. Assemble forecast ensemble variable into `start_year x lead_time x lat x lon` array. \n",
    "1. Compute ensemble mean\n",
    "1. Filter start dates outside of the \"verification window\"\n",
    "1. Compute mean over start dates as a function of lead time: this is the \"drift\"\n",
    "1. Subtract drift from full field\n",
    "\n",
    "\n",
    "First step, generate a `start_year` coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (S: 5)>\n",
       "array([2012, 2013, 2014, 2015, 2016], dtype=int32)\n",
       "Dimensions without coordinates: S\n",
       "Attributes:\n",
       "    long_name:  start year"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = xr.DataArray(np.arange(first_start_year,last_start_year+1,1,dtype='int32')+1,\n",
    "                 dims='S',\n",
    "                 attrs={'long_name':'start year'})\n",
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate `lead_time` array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (L: 10)>\n",
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=int32)\n",
       "Dimensions without coordinates: L\n",
       "Attributes:\n",
       "    long_name:  forecast lead"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = xr.DataArray(np.arange(1,n_forecast_lead+1,1,dtype='int32'),\n",
    "                 dims='L',\n",
    "                 attrs={'long_name':'forecast lead'})\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the files for each start year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 files for 5 start years.\n"
     ]
    }
   ],
   "source": [
    "files_by_year = {}\n",
    "n = np.zeros(len(S),dtype='int32')\n",
    "\n",
    "for i in range(0,len(S)):\n",
    "    year = S.values[i]-1\n",
    "    files_by_year[year] = sorted(glob(f'{dir_dple}/monthly/{var}/{case_prefix}.{year}*.nc'))[:n_ensemble_max]                              \n",
    "    n[i] = len(files_by_year[year])\n",
    "    \n",
    "#-- ensure that we have the same number of files for each start year\n",
    "n_ensemble = n[0]\n",
    "np.testing.assert_equal(n_ensemble,n)\n",
    "\n",
    "print(f'{n_ensemble} files for {len(S)} start years.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (L: 10)>\n",
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10], dtype=int32)\n",
       "Dimensions without coordinates: L\n",
       "Attributes:\n",
       "    long_name:  forecast lead"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble forecast ensemble  \n",
    "\n",
    "Generate a `start_year x lead_time x lat x lon` array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b50cfa7a4f49e396f474d0ab9da957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1.96 s, sys: 120 ms, total: 2.08 s\n",
      "Wall time: 3.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "grid_vars = []\n",
    "ds_list = []\n",
    "\n",
    "#-- loop over files for each year\n",
    "for year,files in tqdm(files_by_year.items()):\n",
    "\n",
    "    #-- open the datasets\n",
    "    ds = xr.open_mfdataset(files,\n",
    "                           concat_dim='M',\n",
    "                           decode_times = False, \n",
    "                           decode_coords = False,\n",
    "                           engine='netcdf4')\n",
    "    \n",
    "    #-- store \"grid\" variables, attributes, and encoding\n",
    "    if not grid_vars:\n",
    "        grid_vars = [v for v,da in ds.variables.items() if 'time' not in da.dims]\n",
    "        ds_grid = ds.drop([v for v in ds.variables if v not in grid_vars]).isel(M=0)\n",
    "        dsattrs = ds.attrs\n",
    "        attrs = {v:da.attrs for v,da in ds.variables.items()}\n",
    "        encoding = {v:{key:val for key,val in da.encoding.items() \n",
    "                       if key in ['dtype','_FillValue','missing_value']}\n",
    "                    for v,da in ds.variables.items()}\n",
    "              \n",
    "    #-- fix time and drop extraneous variables\n",
    "    ds = fix_time(ds)\n",
    "    ds = ds.drop([v for v in ds.variables if v not in ds.coords and v != var])\n",
    "    \n",
    "    #-- subset?\n",
    "    if isel:\n",
    "        ds = ds.isel(**isel)\n",
    "        if 'coordinates' in attrs[var]:\n",
    "            del attrs[var]['coordinates']\n",
    "        if 'coordinates' in ds[var].attrs:\n",
    "            del ds[var].attrs['coordinates']    \n",
    "            \n",
    "    #-- compute appropriate average TODO: ds = calc.compute_{freq}_mean(ds).rename({'time':'L'})\n",
    "    ds = ds.groupby('time.year').mean('time').rename({'year':'L'})   \n",
    "    if len(ds.L) == n_forecast_lead+1:\n",
    "        ds = ds.isel(L=slice(1,11))       \n",
    "    ds['L'] = L \n",
    "    \n",
    "    for v in ds.variables:\n",
    "        if v in attrs:\n",
    "            ds[v].attrs = attrs[v]\n",
    "        if v in encoding:\n",
    "            ds[v].encoding = encoding[v]\n",
    "            \n",
    "    ds_list.append(ds)\n",
    "\n",
    "#-- assemble into single dataset\n",
    "ds = xr.concat(ds_list,dim='S')\n",
    "ds['S'] = S\n",
    "\n",
    "#-- rechunk to more suitable sizes\n",
    "new_chunks = {'S':1,'L':len(L),'M':n_ensemble}\n",
    "\n",
    "#if 'z_t' in ds[var].dims:\n",
    "#    new_chunks = {'S':len(S),'L':len(L),'M':n_ensemble,'nlat':16,'nlon':16}\n",
    "    \n",
    "ds = ds.chunk(new_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<xarray.Dataset>\n",
       " Dimensions:       (L: 10, M: 2, lat_aux_grid: 395, moc_z: 61, nlat: 384, nlon: 320, z_t: 60, z_t_150m: 15, z_w: 60, z_w_bot: 60, z_w_top: 60)\n",
       " Coordinates:\n",
       "   * z_t           (z_t) float32 500.0 1500.0 2500.0 ... 512502.8 537500.0\n",
       "   * z_w           (z_w) float32 0.0 1000.0 2000.0 ... 500004.7 525000.94\n",
       "   * moc_z         (moc_z) float32 0.0 1000.0 2000.0 ... 525000.94 549999.06\n",
       "   * z_w_top       (z_w_top) float32 0.0 1000.0 2000.0 ... 500004.7 525000.94\n",
       "   * z_w_bot       (z_w_bot) float32 1000.0 2000.0 3000.0 ... 525000.94 549999.06\n",
       "   * lat_aux_grid  (lat_aux_grid) float32 -79.48815 -78.952896 ... 89.47441 90.0\n",
       "   * z_t_150m      (z_t_150m) float32 500.0 1500.0 2500.0 ... 13500.0 14500.0\n",
       "   * L             (L) int32 1 2 3 4 5 6 7 8 9 10\n",
       " Dimensions without coordinates: M, nlat, nlon\n",
       " Data variables:\n",
       "     O2            (L, M, z_t, nlat, nlon) float32 dask.array<shape=(10, 2, 60, 384, 320), chunksize=(1, 1, 60, 384, 320)>,\n",
       " <xarray.Dataset>\n",
       " Dimensions:       (L: 10, M: 2, lat_aux_grid: 395, moc_z: 61, nlat: 384, nlon: 320, z_t: 60, z_t_150m: 15, z_w: 60, z_w_bot: 60, z_w_top: 60)\n",
       " Coordinates:\n",
       "   * z_t           (z_t) float32 500.0 1500.0 2500.0 ... 512502.8 537500.0\n",
       "   * z_w           (z_w) float32 0.0 1000.0 2000.0 ... 500004.7 525000.94\n",
       "   * moc_z         (moc_z) float32 0.0 1000.0 2000.0 ... 525000.94 549999.06\n",
       "   * z_w_top       (z_w_top) float32 0.0 1000.0 2000.0 ... 500004.7 525000.94\n",
       "   * z_w_bot       (z_w_bot) float32 1000.0 2000.0 3000.0 ... 525000.94 549999.06\n",
       "   * lat_aux_grid  (lat_aux_grid) float32 -79.48815 -78.952896 ... 89.47441 90.0\n",
       "   * z_t_150m      (z_t_150m) float32 500.0 1500.0 2500.0 ... 13500.0 14500.0\n",
       "   * L             (L) int32 1 2 3 4 5 6 7 8 9 10\n",
       " Dimensions without coordinates: M, nlat, nlon\n",
       " Data variables:\n",
       "     O2            (L, M, z_t, nlat, nlon) float32 dask.array<shape=(10, 2, 60, 384, 320), chunksize=(1, 1, 60, 384, 320)>,\n",
       " <xarray.Dataset>\n",
       " Dimensions:       (L: 10, M: 2, lat_aux_grid: 395, moc_z: 61, nlat: 384, nlon: 320, z_t: 60, z_t_150m: 15, z_w: 60, z_w_bot: 60, z_w_top: 60)\n",
       " Coordinates:\n",
       "   * z_t           (z_t) float32 500.0 1500.0 2500.0 ... 512502.8 537500.0\n",
       "   * z_w           (z_w) float32 0.0 1000.0 2000.0 ... 500004.7 525000.94\n",
       "   * moc_z         (moc_z) float32 0.0 1000.0 2000.0 ... 525000.94 549999.06\n",
       "   * z_w_top       (z_w_top) float32 0.0 1000.0 2000.0 ... 500004.7 525000.94\n",
       "   * z_w_bot       (z_w_bot) float32 1000.0 2000.0 3000.0 ... 525000.94 549999.06\n",
       "   * lat_aux_grid  (lat_aux_grid) float32 -79.48815 -78.952896 ... 89.47441 90.0\n",
       "   * z_t_150m      (z_t_150m) float32 500.0 1500.0 2500.0 ... 13500.0 14500.0\n",
       "   * L             (L) int32 1 2 3 4 5 6 7 8 9 10\n",
       " Dimensions without coordinates: M, nlat, nlon\n",
       " Data variables:\n",
       "     O2            (L, M, z_t, nlat, nlon) float32 dask.array<shape=(10, 2, 60, 384, 320), chunksize=(1, 1, 60, 384, 320)>,\n",
       " <xarray.Dataset>\n",
       " Dimensions:       (L: 10, M: 2, lat_aux_grid: 395, moc_z: 61, nlat: 384, nlon: 320, z_t: 60, z_t_150m: 15, z_w: 60, z_w_bot: 60, z_w_top: 60)\n",
       " Coordinates:\n",
       "   * z_t           (z_t) float32 500.0 1500.0 2500.0 ... 512502.8 537500.0\n",
       "   * z_w           (z_w) float32 0.0 1000.0 2000.0 ... 500004.7 525000.94\n",
       "   * moc_z         (moc_z) float32 0.0 1000.0 2000.0 ... 525000.94 549999.06\n",
       "   * z_w_top       (z_w_top) float32 0.0 1000.0 2000.0 ... 500004.7 525000.94\n",
       "   * z_w_bot       (z_w_bot) float32 1000.0 2000.0 3000.0 ... 525000.94 549999.06\n",
       "   * lat_aux_grid  (lat_aux_grid) float32 -79.48815 -78.952896 ... 89.47441 90.0\n",
       "   * z_t_150m      (z_t_150m) float32 500.0 1500.0 2500.0 ... 13500.0 14500.0\n",
       "   * L             (L) int32 1 2 3 4 5 6 7 8 9 10\n",
       " Dimensions without coordinates: M, nlat, nlon\n",
       " Data variables:\n",
       "     O2            (L, M, z_t, nlat, nlon) float32 dask.array<shape=(10, 2, 60, 384, 320), chunksize=(1, 1, 60, 384, 320)>,\n",
       " <xarray.Dataset>\n",
       " Dimensions:       (L: 10, M: 2, lat_aux_grid: 395, moc_z: 61, nlat: 384, nlon: 320, z_t: 60, z_t_150m: 15, z_w: 60, z_w_bot: 60, z_w_top: 60)\n",
       " Coordinates:\n",
       "   * z_t           (z_t) float32 500.0 1500.0 2500.0 ... 512502.8 537500.0\n",
       "   * z_w           (z_w) float32 0.0 1000.0 2000.0 ... 500004.7 525000.94\n",
       "   * moc_z         (moc_z) float32 0.0 1000.0 2000.0 ... 525000.94 549999.06\n",
       "   * z_w_top       (z_w_top) float32 0.0 1000.0 2000.0 ... 500004.7 525000.94\n",
       "   * z_w_bot       (z_w_bot) float32 1000.0 2000.0 3000.0 ... 525000.94 549999.06\n",
       "   * lat_aux_grid  (lat_aux_grid) float32 -79.48815 -78.952896 ... 89.47441 90.0\n",
       "   * z_t_150m      (z_t_150m) float32 500.0 1500.0 2500.0 ... 13500.0 14500.0\n",
       "   * L             (L) int32 1 2 3 4 5 6 7 8 9 10\n",
       " Dimensions without coordinates: M, nlat, nlon\n",
       " Data variables:\n",
       "     O2            (L, M, z_t, nlat, nlon) float32 dask.array<shape=(10, 2, 60, 384, 320), chunksize=(1, 1, 60, 384, 320)>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:       (L: 10, M: 2, S: 5, lat_aux_grid: 395, moc_z: 61, nlat: 384, nlon: 320, z_t: 60, z_t_150m: 15, z_w: 60, z_w_bot: 60, z_w_top: 60)\n",
       "Coordinates:\n",
       "  * z_t           (z_t) float32 500.0 1500.0 2500.0 ... 512502.8 537500.0\n",
       "  * z_w           (z_w) float32 0.0 1000.0 2000.0 ... 500004.7 525000.94\n",
       "  * moc_z         (moc_z) float32 0.0 1000.0 2000.0 ... 525000.94 549999.06\n",
       "  * z_w_top       (z_w_top) float32 0.0 1000.0 2000.0 ... 500004.7 525000.94\n",
       "  * z_w_bot       (z_w_bot) float32 1000.0 2000.0 3000.0 ... 525000.94 549999.06\n",
       "  * lat_aux_grid  (lat_aux_grid) float32 -79.48815 -78.952896 ... 89.47441 90.0\n",
       "  * z_t_150m      (z_t_150m) float32 500.0 1500.0 2500.0 ... 13500.0 14500.0\n",
       "  * L             (L) int32 1 2 3 4 5 6 7 8 9 10\n",
       "  * S             (S) int32 2012 2013 2014 2015 2016\n",
       "Dimensions without coordinates: M, nlat, nlon\n",
       "Data variables:\n",
       "    O2            (S, L, M, z_t, nlat, nlon) float32 dask.array<shape=(5, 10, 2, 60, 384, 320), chunksize=(1, 10, 2, 60, 384, 320)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `persist` on this dataset to load it into distributed memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xarray.Dataset {\n",
      "dimensions:\n",
      "\tL = 10 ;\n",
      "\tM = 2 ;\n",
      "\tS = 5 ;\n",
      "\tlat_aux_grid = 395 ;\n",
      "\tmoc_z = 61 ;\n",
      "\tnlat = 384 ;\n",
      "\tnlon = 320 ;\n",
      "\tz_t = 60 ;\n",
      "\tz_t_150m = 15 ;\n",
      "\tz_w = 60 ;\n",
      "\tz_w_bot = 60 ;\n",
      "\tz_w_top = 60 ;\n",
      "\n",
      "variables:\n",
      "\tfloat32 z_t(z_t) ;\n",
      "\t\tz_t:units = centimeters ;\n",
      "\t\tz_t:long_name = depth from surface to midpoint of layer ;\n",
      "\t\tz_t:valid_min = 500.0 ;\n",
      "\t\tz_t:valid_max = 537500.0 ;\n",
      "\t\tz_t:positive = down ;\n",
      "\tfloat32 z_w(z_w) ;\n",
      "\t\tz_w:units = centimeters ;\n",
      "\t\tz_w:long_name = depth from surface to top of layer ;\n",
      "\t\tz_w:valid_min = 0.0 ;\n",
      "\t\tz_w:valid_max = 525000.9375 ;\n",
      "\t\tz_w:positive = down ;\n",
      "\tfloat32 moc_z(moc_z) ;\n",
      "\t\tmoc_z:units = centimeters ;\n",
      "\t\tmoc_z:long_name = depth from surface to top of layer ;\n",
      "\t\tmoc_z:valid_min = 0.0 ;\n",
      "\t\tmoc_z:valid_max = 549999.0625 ;\n",
      "\t\tmoc_z:positive = down ;\n",
      "\tfloat32 z_w_top(z_w_top) ;\n",
      "\t\tz_w_top:units = centimeters ;\n",
      "\t\tz_w_top:long_name = depth from surface to top of layer ;\n",
      "\t\tz_w_top:valid_min = 0.0 ;\n",
      "\t\tz_w_top:valid_max = 525000.9375 ;\n",
      "\t\tz_w_top:positive = down ;\n",
      "\tfloat32 z_w_bot(z_w_bot) ;\n",
      "\t\tz_w_bot:units = centimeters ;\n",
      "\t\tz_w_bot:long_name = depth from surface to bottom of layer ;\n",
      "\t\tz_w_bot:valid_min = 1000.0 ;\n",
      "\t\tz_w_bot:valid_max = 549999.0625 ;\n",
      "\t\tz_w_bot:positive = down ;\n",
      "\tfloat32 lat_aux_grid(lat_aux_grid) ;\n",
      "\t\tlat_aux_grid:units = degrees_north ;\n",
      "\t\tlat_aux_grid:long_name = latitude grid for transport diagnostics ;\n",
      "\t\tlat_aux_grid:valid_min = -79.48815155029297 ;\n",
      "\t\tlat_aux_grid:valid_max = 90.0 ;\n",
      "\tfloat32 z_t_150m(z_t_150m) ;\n",
      "\t\tz_t_150m:units = centimeters ;\n",
      "\t\tz_t_150m:long_name = depth from surface to midpoint of layer ;\n",
      "\t\tz_t_150m:valid_min = 500.0 ;\n",
      "\t\tz_t_150m:valid_max = 14500.0 ;\n",
      "\t\tz_t_150m:positive = down ;\n",
      "\tint32 L(L) ;\n",
      "\t\tL:long_name = forecast lead ;\n",
      "\tfloat32 O2(S, L, M, z_t, nlat, nlon) ;\n",
      "\t\tO2:coordinates = TLONG TLAT z_t time ;\n",
      "\t\tO2:grid_loc = 3111 ;\n",
      "\t\tO2:long_name = Dissolved Oxygen ;\n",
      "\t\tO2:cell_methods = time: mean ;\n",
      "\t\tO2:units = mmol/m^3 ;\n",
      "\tint32 S(S) ;\n",
      "\t\tS:long_name = start year ;\n",
      "\n",
      "// global attributes:\n",
      "}CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 237 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#ds = ds.persist()\n",
    "#wait(ds)\n",
    "ds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate a `verification_time` matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray (S: 5, L: 10)>\n",
       "array([[2012.5, 2013.5, 2014.5, 2015.5, 2016.5, 2017.5, 2018.5, 2019.5,\n",
       "        2020.5, 2021.5],\n",
       "       [2013.5, 2014.5, 2015.5, 2016.5, 2017.5, 2018.5, 2019.5, 2020.5,\n",
       "        2021.5, 2022.5],\n",
       "       [2014.5, 2015.5, 2016.5, 2017.5, 2018.5, 2019.5, 2020.5, 2021.5,\n",
       "        2022.5, 2023.5],\n",
       "       [2015.5, 2016.5, 2017.5, 2018.5, 2019.5, 2020.5, 2021.5, 2022.5,\n",
       "        2023.5, 2024.5],\n",
       "       [2016.5, 2017.5, 2018.5, 2019.5, 2020.5, 2021.5, 2022.5, 2023.5,\n",
       "        2024.5, 2025.5]])\n",
       "Dimensions without coordinates: S, L"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verification_time = S + 0.5 + L - 1\n",
    "verification_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute ensemble mean across forecast ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:       (L: 10, S: 5, lat_aux_grid: 395, moc_z: 61, nlat: 384, nlon: 320, z_t: 60, z_t_150m: 15, z_w: 60, z_w_bot: 60, z_w_top: 60)\n",
       "Coordinates:\n",
       "  * z_t           (z_t) float32 500.0 1500.0 2500.0 ... 512502.8 537500.0\n",
       "  * z_w           (z_w) float32 0.0 1000.0 2000.0 ... 500004.7 525000.94\n",
       "  * moc_z         (moc_z) float32 0.0 1000.0 2000.0 ... 525000.94 549999.06\n",
       "  * z_w_top       (z_w_top) float32 0.0 1000.0 2000.0 ... 500004.7 525000.94\n",
       "  * z_w_bot       (z_w_bot) float32 1000.0 2000.0 3000.0 ... 525000.94 549999.06\n",
       "  * lat_aux_grid  (lat_aux_grid) float32 -79.48815 -78.952896 ... 89.47441 90.0\n",
       "  * z_t_150m      (z_t_150m) float32 500.0 1500.0 2500.0 ... 13500.0 14500.0\n",
       "  * L             (L) int32 1 2 3 4 5 6 7 8 9 10\n",
       "  * S             (S) int32 2012 2013 2014 2015 2016\n",
       "Dimensions without coordinates: nlat, nlon\n",
       "Data variables:\n",
       "    O2            (S, L, z_t, nlat, nlon) float32 dask.array<shape=(5, 10, 60, 384, 320), chunksize=(1, 10, 60, 384, 320)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dse = ds.mean('M')\n",
    "dse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute \"drift\"\n",
    "Filter start dates outside of the \"verification window\" \n",
    "Compute mean over start dates as a function of lead time: this is the \"drift\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:       (L: 10, lat_aux_grid: 395, moc_z: 61, nlat: 384, nlon: 320, z_t: 60, z_t_150m: 15, z_w: 60, z_w_bot: 60, z_w_top: 60)\n",
       "Coordinates:\n",
       "  * L             (L) int64 1 2 3 4 5 6 7 8 9 10\n",
       "  * z_t           (z_t) float32 500.0 1500.0 2500.0 ... 512502.8 537500.0\n",
       "  * z_w           (z_w) float32 0.0 1000.0 2000.0 ... 500004.7 525000.94\n",
       "  * moc_z         (moc_z) float32 0.0 1000.0 2000.0 ... 525000.94 549999.06\n",
       "  * z_w_top       (z_w_top) float32 0.0 1000.0 2000.0 ... 500004.7 525000.94\n",
       "  * z_w_bot       (z_w_bot) float32 1000.0 2000.0 3000.0 ... 525000.94 549999.06\n",
       "  * lat_aux_grid  (lat_aux_grid) float32 -79.48815 -78.952896 ... 89.47441 90.0\n",
       "  * z_t_150m      (z_t_150m) float32 500.0 1500.0 2500.0 ... 13500.0 14500.0\n",
       "Dimensions without coordinates: nlat, nlon\n",
       "Data variables:\n",
       "    O2            (L, z_t, nlat, nlon) float32 dask.array<shape=(10, 60, 384, 320), chunksize=(10, 60, 384, 320)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drift = dse.where((first_clim_year<verification_time) & \n",
    "                  (verification_time<last_clim_year+1) ).mean('S')\n",
    "\n",
    "for v in drift.variables:\n",
    "    if v in attrs:\n",
    "        drift[v].attrs = attrs[v]\n",
    "    if v in encoding:\n",
    "        drift[v].encoding = encoding[v]\n",
    "        \n",
    "drift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute bias correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:       (L: 10, M: 2, S: 5, lat_aux_grid: 395, moc_z: 61, nlat: 384, nlon: 320, z_t: 60, z_t_150m: 15, z_w: 60, z_w_bot: 60, z_w_top: 60)\n",
       "Coordinates:\n",
       "  * z_t           (z_t) float32 500.0 1500.0 2500.0 ... 512502.8 537500.0\n",
       "  * z_w           (z_w) float32 0.0 1000.0 2000.0 ... 500004.7 525000.94\n",
       "  * moc_z         (moc_z) float32 0.0 1000.0 2000.0 ... 525000.94 549999.06\n",
       "  * z_w_top       (z_w_top) float32 0.0 1000.0 2000.0 ... 500004.7 525000.94\n",
       "  * z_w_bot       (z_w_bot) float32 1000.0 2000.0 3000.0 ... 525000.94 549999.06\n",
       "  * lat_aux_grid  (lat_aux_grid) float32 -79.48815 -78.952896 ... 89.47441 90.0\n",
       "  * z_t_150m      (z_t_150m) float32 500.0 1500.0 2500.0 ... 13500.0 14500.0\n",
       "  * L             (L) int32 1 2 3 4 5 6 7 8 9 10\n",
       "  * S             (S) int32 2012 2013 2014 2015 2016\n",
       "Dimensions without coordinates: M, nlat, nlon\n",
       "Data variables:\n",
       "    O2            (S, L, M, z_t, nlat, nlon) float32 dask.array<shape=(5, 10, 2, 60, 384, 320), chunksize=(1, 10, 2, 60, 384, 320)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anom = ds - drift\n",
    "\n",
    "for v in anom.variables:\n",
    "    if v in attrs:\n",
    "        anom[v].attrs = attrs[v]\n",
    "    if v in encoding:\n",
    "        anom[v].encoding = encoding[v]\n",
    "anom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write output to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsattrs['history'] = f'created by {USER} on {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing /glade/scratch/abanihi/calcs/o2-prediction/CESM-DP-LE.O2.ann.mean.nc\n",
      "CPU times: user 7.77 s, sys: 596 ms, total: 8.37 s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if var != varout and rename_var:\n",
    "    ds.rename({var:varout},inplace=True)\n",
    "    \n",
    "write_output(ds, \n",
    "             file_out = file_out['mean'], \n",
    "             attrs = dsattrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsattrs['climatology'] = f'{first_clim_year:d}-{last_clim_year}, computed separately for each lead time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing /glade/scratch/abanihi/calcs/o2-prediction/CESM-DP-LE.O2.ann.mean.anom.nc\n",
      "CPU times: user 6.98 s, sys: 556 ms, total: 7.54 s\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if rename_var:\n",
    "    anom.rename({var:'anom'},inplace=True)\n",
    "\n",
    "write_output(anom, \n",
    "             file_out = file_out['anom'], \n",
    "             attrs = dsattrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing /glade/scratch/abanihi/calcs/o2-prediction/CESM-DP-LE.O2.ann.mean.drift.nc\n",
      "CPU times: user 6.7 s, sys: 420 ms, total: 7.12 s\n",
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if rename_var:\n",
    "    drift.rename({var:'climo'},inplace=True)\n",
    "\n",
    "write_output(drift, \n",
    "             file_out = file_out['drift'], \n",
    "             attrs = dsattrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size in GB 2.95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('dataset size in GB {:0.2f}\\n'.format(ds.nbytes / 1e9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pip install version_information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "Software versions": [
        {
         "module": "Python",
         "version": "3.6.6 64bit [GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]"
        },
        {
         "module": "IPython",
         "version": "7.0.1"
        },
        {
         "module": "OS",
         "version": "Linux 3.12.62 60.64.8 default x86_64 with SuSE 12 x86_64"
        },
        {
         "module": "netcdf4",
         "version": "1.4.1"
        },
        {
         "module": "xarray",
         "version": "0.10.9"
        },
        {
         "module": "dask",
         "version": "0.19.4"
        },
        {
         "module": "numpy",
         "version": "1.15.2"
        },
        {
         "module": "zarr",
         "version": "2.2.0"
        }
       ]
      },
      "text/html": [
       "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.6.6 64bit [GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]</td></tr><tr><td>IPython</td><td>7.0.1</td></tr><tr><td>OS</td><td>Linux 3.12.62 60.64.8 default x86_64 with SuSE 12 x86_64</td></tr><tr><td>netcdf4</td><td>1.4.1</td></tr><tr><td>xarray</td><td>0.10.9</td></tr><tr><td>dask</td><td>0.19.4</td></tr><tr><td>numpy</td><td>1.15.2</td></tr><tr><td>zarr</td><td>2.2.0</td></tr><tr><td colspan='2'>Mon Oct 29 00:26:37 2018 MDT</td></tr></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{|l|l|}\\hline\n",
       "{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\n",
       "Python & 3.6.6 64bit [GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] \\\\ \\hline\n",
       "IPython & 7.0.1 \\\\ \\hline\n",
       "OS & Linux 3.12.62 60.64.8 default x86\\_64 with SuSE 12 x86\\_64 \\\\ \\hline\n",
       "netcdf4 & 1.4.1 \\\\ \\hline\n",
       "xarray & 0.10.9 \\\\ \\hline\n",
       "dask & 0.19.4 \\\\ \\hline\n",
       "numpy & 1.15.2 \\\\ \\hline\n",
       "zarr & 2.2.0 \\\\ \\hline\n",
       "\\hline \\multicolumn{2}{|l|}{Mon Oct 29 00:26:37 2018 MDT} \\\\ \\hline\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "Software versions\n",
       "Python 3.6.6 64bit [GCC 4.8.2 20140120 (Red Hat 4.8.2-15)]\n",
       "IPython 7.0.1\n",
       "OS Linux 3.12.62 60.64.8 default x86_64 with SuSE 12 x86_64\n",
       "netcdf4 1.4.1\n",
       "xarray 0.10.9\n",
       "dask 0.19.4\n",
       "numpy 1.15.2\n",
       "zarr 2.2.0\n",
       "Mon Oct 29 00:26:37 2018 MDT"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext version_information\n",
    "%version_information netcdf4, xarray, dask, numpy, zarr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
